{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ck-N4OzQqBS1"
      },
      "source": [
        "# Urban Computing - Lab 1\n",
        "This lab introduces Python for scientific programming, shows you examples on how to handle API access and visualization for location information, how to generate simulated trajectory data, as well as a number of tips for optimizing your code. We assume prior familiarity with numpy and matplotlib. The material in this lab serves as an important basis for your assignments. Therefore, make sure to familiarize yourself with them before the first assignment is due. This lab is not going to be graded and we will make the solutions available later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrHLYJbGqBS2"
      },
      "source": [
        "This lab contains four parts: \n",
        "\n",
        "- Part I Numpy and basic scientific programming\n",
        "- Part II Maps: OpenRoute Service and Folium\n",
        "- Part III Foursquare API and Pandas dataframes\n",
        "- Part IV Troubleshooting poor performance in Pandas (and Numpy)   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJMRYap7_suL"
      },
      "source": [
        "But before we start with the execersises here is some information on how to write clean code in Python:\n",
        "## Code style\n",
        "Unlike for example C++ where people just can't agree exactly what the best way is to place brackets, for Python there is just one key standard for how good clean code looks like: PEP8. PEP stands for Python Enhancement Proposal. You can find the official description [here](https://www.python.org/dev/peps/pep-0008/). A bit lighter reading version that gives more explanation is [here](https://realpython.com/python-pep8/).\n",
        "\n",
        "Writing clean code is important. It's very common that you find yourself doing some project and thinking, \"hey, I solved this little problem in another problem two months ago, how did I do that again?\" If you wrote nice clean code, it's easy to look up how you did it. Likewise, if you're working together with people, it's easier if you understand what others are doing. Adhering (sensibly) to PEP8 makes this easy.\n",
        "\n",
        "The original PEP8 standard assumes a 80-character line limit, but with modern screens, it's reasonable to assume a 120 character line limit.\n",
        "\n",
        "Python also has a very definite philosophy behind it, and knowing that philosophy will make it easier to understand how a lot of things are designed in Python. It's called The Zen of Python, officially documented as PEP20, and can be found [here](https://www.python.org/dev/peps/pep-0020/) and there are many essays explaining these 19 statements, for example [here](https://inventwithpython.com/blog/2018/08/17/the-zen-of-python-explained/). Note though that every article explaining them is subtly different. They're intended to make you think, not blindly follow :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAVJYbtEqBS2"
      },
      "source": [
        "## Part I  Numpy and basic scientific programming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fpw5qXi4qBS3"
      },
      "source": [
        "#### Loop Plot\n",
        "The code below constructs a live-plot visualization inside a for-loop. <br>\n",
        "It gradually draws $x$,$y$ data points derived from the following trigonometric equations: $$x(t)=t-1.6\\cdot cos(24t)\\\\y(t)=t-1.6\\cdot sin(25t)\\\\ \\text{where}\\ t \\in \\{0.01, 0.02, 0.03, ..., 0.18\\}$$\n",
        "Try to change some of the parameters (e.g. `hi_b`, `lo_b`,...) and see the differences.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-Q27yBAqBS3",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def fx(t):\n",
        "    return t - 1.6 * np.cos(24 * t)\n",
        "\n",
        "\n",
        "def fy(t):\n",
        "    return t - 1.6 * np.sin(25 * t)\n",
        "\n",
        "\n",
        "# Step Parameters for vector t\n",
        "hi_b = 0.18\n",
        "lo_b = 0.01\n",
        "by = 0.02\n",
        "step = np.arange(lo_b, hi_b, by)\n",
        "step = np.flip(step, axis=0)\n",
        "\n",
        "# Slightly perturb\n",
        "step = np.array([st + ((np.random.rand() - 0.5)/100) for st in step])\n",
        "\n",
        "# Live-Plot inside the for-loop\n",
        "for st in step:\n",
        "    print(st)\n",
        "    t = np.arange(0, 10, st)\n",
        "    x = fx(t)\n",
        "    y = fy(t)\n",
        "    plt.figure()\n",
        "    plt.plot(x, y)\n",
        "    plt.pause(0.4)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-wLDNqiqBS7"
      },
      "source": [
        "### Exercise 1 - Plots\n",
        "Use the previous information and visualize your own live-plot using the following trigonometric equations:\n",
        "$$x(u) = sin(33u)cos(9u)\\\\\n",
        "y(u) = sin(40u)sin(7u)$$\n",
        "A correct final image, if everything was written correctly, is something like the following:\n",
        "\n",
        "<img src=\"data/plt.png\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgGmEZUZqBS8",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function for generating x(u)\n",
        "# Code here:\n",
        "def fx(u):\n",
        "    return np.sin(33*u) * np.cos(9*u)\n",
        "\n",
        "\n",
        "# Function for generating y(u)\n",
        "# Code here:\n",
        "def fy(u):\n",
        "    return np.sin(40*u) * np.sin(7*u)\n",
        "\n",
        "\n",
        "# Generate vector u\n",
        "hi_b = 0.018\n",
        "lo_b = 0.001\n",
        "by = 0.002\n",
        "step = np.arange(lo_b, hi_b, by)\n",
        "step = np.flip(step, axis=0)\n",
        "\n",
        "# # Slightly perturb\n",
        "# step = np.array([st + ((np.random.rand() - 0.5)/100) for st in step])\n",
        "\n",
        "# Live-Plot inside the for-loop\n",
        "plt.figure()\n",
        "for st in step:\n",
        "    t = np.arange(-1, 1, st)\n",
        "    x = fx(t)\n",
        "    y = fy(t)\n",
        "    plt.plot(x, y, color='tab:blue')\n",
        "    plt.plot(x, -y, color='tab:blue')\n",
        "    # plt.pause(0.3)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9alzXpaqBTB"
      },
      "source": [
        "## Part II  Maps: OpenRoute Service and Folium "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGtAiWUrqBTB"
      },
      "source": [
        "#### Download APIs for Python\n",
        "- #### Some basic API's: \n",
        "    - The `openrouteservice` [(documentation)](https://openrouteservice-py.readthedocs.io/en/latest/) library is already installed in the development container. The current package provides a functional python-API for requesting geographical data directly from openrouteservice. You will have to create an account and get the API_KEY in order to be able to work with this library. You can find more information in the website [openrouteservice.org](https://openrouteservice.org/services/) where you can also create such an account, read the documentation and play around with online data request forms.\n",
        "\n",
        "    - The second library is called `folium` [(documentation)](https://python-visualization.github.io/folium/) and it provides API for map visualization. It has already been installed in the development container."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMPjE9l8qBTC"
      },
      "source": [
        " - #### Using `openrouteservice`\n",
        "     - Import the library, provide some coordinates and request the route. \n",
        "     - Try to understand the content of the routes dictionary.\n",
        "     - Note that we always have to provide the api_key when we generate a client."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGYyPwJhqBTC"
      },
      "outputs": [],
      "source": [
        "import openrouteservice as opn\n",
        "from openrouteservice.directions import directions\n",
        "\n",
        "api_key = \"5b3ce3597851110001cf62482fff3e10b7da47968742a37590c03b6d\"\n",
        "coords = ((8.34234, 48.23424), (8.34423, 48.26424))\n",
        "\n",
        "client = opn.Client(key=api_key)  # Specify your personal API key\n",
        "routes = directions(client, coords)\n",
        "routes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fk_nbxBNqBTE"
      },
      "source": [
        "- #### Decode a PolyLine\n",
        "The output of the `directions` function is a json file by default. We can decode it to a simple dictionary which, in this case, is a GeoJSON geometry. We can simply do the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBS9zZjOqBTF"
      },
      "outputs": [],
      "source": [
        "import openrouteservice as opn\n",
        "from openrouteservice.directions import directions\n",
        "from openrouteservice import convert\n",
        "\n",
        "api_key = \"5b3ce3597851110001cf62482fff3e10b7da47968742a37590c03b6d\"\n",
        "coords = ((8.34234, 48.23424), (8.34423, 48.26424))\n",
        "\n",
        "client = opn.Client(key=api_key)  # Specify your personal API key\n",
        "\n",
        "# decode_polyline needs the geometry only\n",
        "geometry = client.directions(coords)[\"routes\"][0][\"geometry\"]\n",
        "\n",
        "decoded = convert.decode_polyline(geometry)\n",
        "\n",
        "decoded  # this has only the coordinates for the route.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvBL-PxMqBTK"
      },
      "source": [
        "- #### Visualize the maps\n",
        "    - Import `folium` library\n",
        "    - Get familiar using the [link](https://python-visualization.github.io/folium/quickstart.html)\n",
        "    - Try to plot a map using the the coordinates: 52.169709,4.457111 \n",
        "    - Is the place on the map familiar to you?\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkCfhxOaqBTK",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "import folium\n",
        "\n",
        "# Set up folium map\n",
        "map1 = folium.Map(location=([52.169709, 4.457111]), zoom_start=17)\n",
        "map1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19O_KhA_qBTN"
      },
      "source": [
        "- #### Visualize specific locations\n",
        "    1. Plot the map around the previous location.\n",
        "    2. Specify university buildings.\n",
        "    3. Request isochrones within some minutes walking from each building.\n",
        "    4. Draw the radius of isochrones (note the reversed coordinates).\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UbM_RXyqBTN",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "import openrouteservice as opn\n",
        "import folium\n",
        "\n",
        "# Specify your personal API key\n",
        "api_key = \"5b3ce3597851110001cf62482fff3e10b7da47968742a37590c03b6d\"\n",
        "clnt = opn.Client(key=api_key)\n",
        "\n",
        "# 1. Map\n",
        "map1 = folium.Map(location=([52.169709, 4.457111]), zoom_start=16)\n",
        "\n",
        "# 2. Set up the building dictionary with real coordinates\n",
        "bld_dict = {\n",
        "    \"Archeologie\": {\"location\": [4.464366, 52.165078]},\n",
        "    \"Gorlaeus\": {\"location\": [4.459992, 52.167412]},\n",
        "    \"Jansen\": {\"location\": [4.455754, 52.167730]},\n",
        "    \"Chemistry\": {\"location\": [4.459306, 52.168101]},\n",
        "}\n",
        "\n",
        "# 3. Request of isochrones with 2 minute footwalk.\n",
        "params_iso = {\n",
        "    \"profile\": \"cycling-road\",\n",
        "    \"intervals\": [120],  # 120/60 = 2 mins\n",
        "    \"segments\": 120,\n",
        "    \"attributes\": [\"total_pop\"],  # Get population count for isochrones\n",
        "}\n",
        "\n",
        "# 4. Add the isochrones to the map and visualize it\n",
        "for name, bld in bld_dict.items():\n",
        "    # Add buildings coords to request parameters\n",
        "    params_iso[\"locations\"] = [bld[\"location\"]]\n",
        "\n",
        "    # Perform isochrone request\n",
        "    bld[\"iso\"] = clnt.isochrones(**params_iso)\n",
        "    folium.features.GeoJson(bld[\"iso\"]).add_to(map1)\n",
        "\n",
        "    # Create a map icon\n",
        "    icon = folium.Icon(\n",
        "        color=\"lightgray\",\n",
        "        icon_color=\"#cc0000\",\n",
        "        icon=\"home\",\n",
        "        prefix=\"fa\",\n",
        "    )\n",
        "\n",
        "    # reverse coords due to folium syntax:Latitude/Longitude of Map (Northing, Easting).\n",
        "    # Add marker to map.\n",
        "    reversed_coords = list(reversed(bld[\"location\"]))\n",
        "    marker = folium.map.Marker(reversed_coords, icon=icon, popup=name)\n",
        "    marker.add_to(map1)\n",
        "\n",
        "map1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77xilzFzqBTR"
      },
      "source": [
        "- #### Simulating trajectories of horseback riders\n",
        "Near LIACS, on the Wassenaarseweg, there are stables where children go for horseback riding. Here, we would like to simulate the daily rounds that the horses do. Such simulations can help when assessing how robust algorithms are to noise or how well they perform in general. We will use simulated trajectories in the first assignment.\n",
        "While there are other ways to get an object with timestamped longitudes and latitudes, we are presenting here a basic version. Feel free to increase the difficulty or add more cycles (i.e. additional daily or weekly trajectories) as you wish.\n",
        "\n",
        "The only movement that the horses do is one round in the park, the rest of the time, they stay in the stables. There are two locations in the park where the children will take a break and have a snack for 30 min. Try to generate and visualize these simulated trajectories.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPQc57ZNqBTS"
      },
      "outputs": [],
      "source": [
        "import openrouteservice as opn\n",
        "from openrouteservice.directions import directions\n",
        "from openrouteservice import convert\n",
        "import numpy as np\n",
        "import folium\n",
        "import time\n",
        "\n",
        "\n",
        "# A) coordinates of the stables and the two picnic sites:\n",
        "\n",
        "locations = dict(\n",
        "    ridingschool = [4.467626, 52.170207],\n",
        "    corner_one = [4.474798, 52.172679],\n",
        "    corner_two = [4.475071, 52.169961]\n",
        ")\n",
        "\n",
        "\n",
        "# B) Get the circuit from open route services:\n",
        "\n",
        "api_key = \"5b3ce3597851110001cf62482fff3e10b7da47968742a37590c03b6d\"\n",
        "client = opn.Client(key=api_key)\n",
        "\n",
        "\n",
        "# C) Create an object with timestamped long/lat\n",
        "# Say it takes 20 min (the children are very slow) to get from one fix point to the next:\n",
        "# Remember that the break times are 30 min long\n",
        "# Fuse timestamps, long, lat into one object\n",
        "\n",
        "legs = [\n",
        "    {'start_c': locations['ridingschool'], 'end_c': locations['corner_one'], 'trajectory': [], 'timestamps': []},\n",
        "    {'start_c': locations['corner_one'], 'end_c': locations['corner_two'], 'trajectory': [], 'timestamps': []},\n",
        "    {'start_c': locations['corner_two'], 'end_c': locations['ridingschool'], 'trajectory': [], 'timestamps': []},\n",
        "]\n",
        "\n",
        "routes = {}\n",
        "\n",
        "for i, leg in enumerate(legs):\n",
        "\n",
        "    geometry = client.directions(\n",
        "        (leg['start_c'], leg['end_c']),\n",
        "        profile = 'foot-walking'\n",
        "    )[\"routes\"][0][\"geometry\"]\n",
        "    trajectory = convert.decode_polyline(geometry)['coordinates']\n",
        "    timestamps = np.arange(i*50, i*50+20, 20/(len(trajectory)+1))[1:]\n",
        "    timestamps = [time.strftime('%H:%M:%S', time.gmtime(ts*60)) for ts in timestamps]\n",
        "    leg.update({'trajectory': trajectory, 'timestamps': timestamps})\n",
        "\n",
        "\n",
        "# D) Fill the rest of the day with \"Staying at stables\" ???\n",
        "\n",
        "\n",
        "# E) Can you visualize the trajectory?\n",
        "# Set up folium map\n",
        "\n",
        "center = np.average([coords for coords in locations.values()], axis = 0)\n",
        "map1 = folium.Map(list(reversed(center)), zoom_start = 16)\n",
        "\n",
        "\n",
        "\n",
        "for leg in legs:\n",
        "    \n",
        "    for coords, timestamp in zip(leg['trajectory'], leg['timestamps']):\n",
        "        icon = folium.Icon(\n",
        "            color = 'lightgray',\n",
        "            icon_color = '#cc0000',\n",
        "            icon = 'map-pin',\n",
        "            prefix = 'fa'\n",
        "        )\n",
        "        marker = folium.map.Marker(list(reversed(coords)), icon = icon, popup = timestamp)\n",
        "        marker.add_to(map1)\n",
        "\n",
        "for name, coords in locations.items():\n",
        "    icon = folium.Icon(\n",
        "            color = 'blue',\n",
        "            icon_color = '#cc0000',\n",
        "            icon = 'home',\n",
        "            prefix = 'fa'\n",
        "    )\n",
        "    marker = folium.map.Marker(list(reversed(coords)), icon = icon, popup = name)\n",
        "    marker.add_to(map1)\n",
        "\n",
        "map1\n",
        "\n",
        "\n",
        "# F) How would you add noise to the trajectory? What kind of noise could occur?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRhpHOtoqBTU"
      },
      "source": [
        "## Part III  Foursquare API and pandas dataframes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Crw66rUZqBTU"
      },
      "source": [
        "- ####  Foursquare API\n",
        "Visit [foursquare](https://developer.foursquare.com/places-api) site and create a developer free account. Start an app and install [foursquare-python-API](https://pypi.org/project/foursquare/) by typing on a terminal: ``` pip install foursquare```, or download it from the site. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLyuJDkNqBTV"
      },
      "source": [
        "The following block of code provides some information about how to use `foursquare-API` in python. All you need is to create a developer's account and connect it using the `client_id` and `client_secret` hash from your application in [foursquare](https://developer.foursquare.com/places-api). Then, you create a client object and use it for data request. Please have a look at the [foursquare-documentation](https://developer.foursquare.com/docs).<br>\n",
        "The specific example requests all bars in specific radius from the Leiden center."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cY0FjF3hqBTV",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import foursquare as fsq\n",
        "\n",
        "# Please copy paste your client_id and client_secret from your web foursquare-app\n",
        "cl_id = \"Use your client_id\"\n",
        "cl_sec = \"Use your client_secret\"\n",
        "\n",
        "# Construct the client object\n",
        "client = fsq.Foursquare(client_id=cl_id, client_secret=cl_sec)\n",
        "\n",
        "# Make a request\n",
        "## Request specificaly for venues near the center of Leiden\n",
        "### ll     --> latitude/longitude\n",
        "### query  --> type of venue\n",
        "### intent --> intent performing search\n",
        "### radius --> Limit results to venues within this\n",
        "### limit  --> Number of results to return, up to 50\n",
        "data = client.venues.search(\n",
        "    params={\n",
        "        \"ll\": \"52.160236, 4.497012\",\n",
        "        \"query\": \"bar\",\n",
        "        \"intent\": \"browse\",\n",
        "        \"radius\": 300,\n",
        "        \"limit\": 50,\n",
        "    }\n",
        ")\n",
        "print(\"Bars near my house: \" + str(len(data[\"venues\"])))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAqZau0cqBTX"
      },
      "source": [
        "#### Exercise 2 - Foursquare\n",
        "#### a. Index the requested dictionary and convert it into a pandas DataFrame\n",
        "1. Request from foursquare all possible venues within 3000 meters radius of the Leiden city center. Find the coordinates of Leiden Center from google-maps or any other map application.\n",
        "2. Make a function which takes a dictionary as input.\n",
        "2. Index the dictionary and get interesting information.\n",
        "3. Make a DataFrame from that dictionary with the following information in columns: name, latitude, longitude, distance, address, genre (pluralName).\n",
        "4. You noticed that in many cases some of the information is missing. Make sure that your DataFrame is loaded with the correct information from the dictionary where it was possible, anywhere else fill in with 'NA'. \n",
        "5. Return the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3K58xRERqBTY",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import json\n",
        "\n",
        "url = 'https://api.foursquare.com/v3/places/search'\n",
        "\n",
        "cl_id = '5QQCPFS2R045Y4HELFZB2BGI3SZ35WTCFUSEBWTGM3IF1NRT'\n",
        "cl_sec = 'DQPIORSBGEHHRITQPPLKXDSK3PCKGI13GBMEKH5AZK1BJ4YZ'\n",
        "auth = 'fsq30wbR04nEArbJHJA6Yr4/4pznm4ytwBoIRvSY+wCnpcM='\n",
        "\n",
        "headers = dict(\n",
        "    accept = 'application/json',\n",
        "    authorization = auth\n",
        ")\n",
        "\n",
        "params = dict(\n",
        "    client_id = cl_id,\n",
        "    client_secret = cl_sec,\n",
        "    ll = '52.1665,4.4870',\n",
        "    radius = 3000,\n",
        "    limit = 50\n",
        ")\n",
        "\n",
        "response = requests.get(url, headers=headers, params=params)\n",
        "data = response.json()\n",
        "\n",
        "def try_keys(d: dict, keys: list[str]) -> str:\n",
        "    data = d\n",
        "    try:\n",
        "        for key in keys:\n",
        "            data = data[key]\n",
        "    except KeyError:\n",
        "        return 'NA'\n",
        "    return data\n",
        "\n",
        "def venue_scraper(data: dict) -> pd.DataFrame:\n",
        "    results = data['results']\n",
        "    \n",
        "    df = pd.DataFrame(columns=['name', 'latitude', 'longitude', 'distance', 'address', 'genre'])\n",
        "    for i, res in enumerate(data['results']):\n",
        "        name = res['name']\n",
        "        latitude = try_keys(res, ['geocodes', 'main', 'latitude'])\n",
        "        longitude = try_keys(res, ['geocodes', 'main', 'longitude'])\n",
        "        distance = try_keys(res, ['distance'])\n",
        "        address = try_keys(res, ['location', 'address'])\n",
        "        genre = [try_keys(category, ['name']) for category in res['categories']]\n",
        "\n",
        "        df.loc[i] = [name, latitude, longitude, distance, address, genre]\n",
        "    return df\n",
        "\n",
        "venue_scraper(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6GP4JV6qBTa"
      },
      "source": [
        "#### b. Count the frequencies of the different venue types (genres)\n",
        "Search in [pandas-documentation](https://pandas.pydata.org/pandas-docs/stable/reference/index.html) and try to answer the following:\n",
        "1. Count the frequencies of each venue-type.\n",
        "2. Count the number of NA values.\n",
        "3. Find the nearest venue in the center of Leiden.\n",
        "4. Find the most distant venue from the center."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x17f1odnqBTb"
      },
      "outputs": [],
      "source": [
        "venues_df = venue_scraper(data)\n",
        "\n",
        "# Count frequencies\n",
        "frequencies = {}\n",
        "for genres in venues_df['genre']:\n",
        "    for genre in genres:\n",
        "        try:\n",
        "            frequencies[genre] += 1\n",
        "        except KeyError:\n",
        "            frequencies[genre] = 1\n",
        "\n",
        "print(\"Frequencies of Venue-Types\\n\\n\" + str(x))\n",
        "for category, frequency in frequencies.items():\n",
        "    print(category, frequency)\n",
        "\n",
        "# Find NA and count their frequencies\n",
        "NA_count = sum([(c=='NA') for col in venues_df for c in venues_df[col]])\n",
        "print(\"\\nFrequency of NA: \" + str(NA_count))\n",
        "\n",
        "# Nearest venue\n",
        "near_venue = venues_df.sort_values('distance').iloc[0]['name']\n",
        "# Distant venue\n",
        "far_venue = venues_df.sort_values('distance', ascending=False).iloc[0]['name']\n",
        "\n",
        "print(\"\\nClosest Venue: \" + str(near_venue) + \"\\nDistant Venue: \" + str(far_venue))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9AjQ3ptqBTd"
      },
      "source": [
        "#### c. Visualize Frequencies \n",
        "Plot a barplot for venue-types (genre), using pandas and matplotlib, only for frequencies which are:\n",
        "$\\text{genre}\\geq2$. Barplots are suitable graphs for frequencies visualization but this is not the only way. Find another proper way to visualize those frequencies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RqyS5MGPqBTe"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Query the dataframe for values bigger or equal than 2\n",
        "frequencies_df = pd.DataFrame(columns=['category', 'frequency'])\n",
        "for i, (category, frequency) in enumerate(frequencies.items()):\n",
        "    frequencies_df.loc[i] = [category, frequency]\n",
        "frequencies_df = frequencies_df.query('frequency>=2').set_index('category')\n",
        "\n",
        "# First way of plotting frequencies Barplot\n",
        "bar_ax = frequencies_df.plot.bar(rot=45)\n",
        "plt.xticks(horizontalalignment='right')\n",
        "\n",
        "# Second way of plotting frequencies\n",
        "pie_ax = frequencies_df.plot.pie(y='frequency')\n",
        "pie_ax.get_legend().remove()\n",
        "pie_ax.set_ylabel('');\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_ZRGcr4qBTh"
      },
      "source": [
        "#### d. Visualize locations in maps\n",
        "Use all previous information and draw pins for all venues in the center of leiden.<br>\n",
        "Open a map using folium and initialize it with Leiden center coordinates: 52.159536, 4.491366.<br> Then, plot all the venues from the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Dcjv6j7qBTh"
      },
      "outputs": [],
      "source": [
        "import folium\n",
        "\n",
        "# Make a list with the coordinates of each location\n",
        "locations = {v['name']: (v['longitude'], v['latitude']) for _, v in venues_df.iterrows()}\n",
        "\n",
        "# Open a map and initialize it with the coordinates\n",
        "map2 = folium.Map(location=[52.159536, 4.491366], zoom_start=15)\n",
        "\n",
        "for name, loc in locations.items():\n",
        "\n",
        "    # Create a map icon\n",
        "    icon = folium.Icon(\n",
        "        color=\"lightgray\",\n",
        "        icon_color=\"#cc0000\",\n",
        "        icon=\"map-pin\",\n",
        "        prefix=\"fa\",\n",
        "    )\n",
        "\n",
        "    # reverse coords due to folium syntax:Latitude/Longitude of Map (Northing, Easting).\n",
        "    # Add marker to map.\n",
        "    reversed_coords = list(reversed(loc))\n",
        "    marker = folium.map.Marker(reversed_coords, icon=icon, popup=name)\n",
        "    marker.add_to(map2)\n",
        "\n",
        "map2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpIShZytojfH"
      },
      "source": [
        "## Part IV Troubleshooting poor Pandas (and Numpy) performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzBCV5xY5kjj"
      },
      "source": [
        "### Profiling and Optimizing Loops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYpzYq-EqFA-"
      },
      "source": [
        "As a data scientist, you're going to work with large amounts of data. Numpy, Pandas and Matplotlib are fully capable of doing a lot of work for you, but it matters a lot if you use them in the right way. If your code takes a lot of time to run, then it's hard to work on it, run tests, improve it, and do interesting experiments. This section of the lab focuses on finding the problem spots and speeding them up.\n",
        "\n",
        "You might think that doing the heavy lifting in a Jupyter Notebook seems a bit strange because usually the more GUI the more performance you sacrifice. However, the cell-based setup of these notebooks actually makes it very easy to zoom in on parts of your program and evaluate their performance, using Jupyter's *magic commands*. We're going to be using two specific commands a lot: `timeit` and `lprun`. `timeit` is already built-in, but `lprun` needs to be installed through `pip`. However, this has already been done in our development container (see also the *\"requirements.txt\"* file),  To use `lprun` we need to load it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8KO9zBernef"
      },
      "source": [
        "This section of the tutorial is inspired largely by a [talk](https://www.youtube.com/watch?v=HN5d490_KKk) given at PyCon '17 by Sofia Heisler. You will need a dataset which (if it wasn't bundled with this lab) can be downloaded [here](https://github.com/s-heisler/pycon2017-optimizing-pandas/blob/master/pyCon%20materials/new_york_hotels.csv)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxVErabcsaMo"
      },
      "outputs": [],
      "source": [
        "# Load our tools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the line profiler extension (lprun)\n",
        "%load_ext line_profiler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpmHzfGIpFv_"
      },
      "outputs": [],
      "source": [
        "# Load and peek at the dataset\n",
        "df = pd.read_csv(\"data/new_york_hotels.csv\", encoding=\"cp1252\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4hwo_qOsuNU"
      },
      "source": [
        "We're going to be comparing these hotels by measuring their [great circle distance](https://en.wikipedia.org/wiki/Great-circle_distance) to a constant location using the [Haversine formula](https://en.wikipedia.org/wiki/Haversine_formula). Our constant location will be a shop called [Brooklyn Superhero Supply Co.](https://www.superherosupplies.com/) which is located at (40.671, -73.985)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tl95MtbjtTy5"
      },
      "outputs": [],
      "source": [
        "# Define a basic Haversine distance formula\n",
        "def haversine(lat1, lon1, lat2, lon2):\n",
        "    MILES = 3959\n",
        "    lat1, lon1, lat2, lon2 = map(np.deg2rad, [lat1, lon1, lat2, lon2])\n",
        "    dlat = lat2 - lat1\n",
        "    dlon = lon2 - lon1\n",
        "    a = np.sin(dlat / 2) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2) ** 2\n",
        "    c = 2 * np.arcsin(np.sqrt(a))\n",
        "    total_miles = MILES * c\n",
        "    return total_miles\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UM29jdeHuUO9"
      },
      "source": [
        "Let's say that we started out with just a simple loop to collect the distances of all these hotels to the shop;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "naPIPKgWxIOz"
      },
      "outputs": [],
      "source": [
        "# Define a function to manually loop over all rows and return a series of distances\n",
        "def haversine_looping(df):\n",
        "    distance_list = []\n",
        "    for i in range(0, len(df)):\n",
        "        d = haversine(40.671, -73.985, df.iloc[i][\"latitude\"], df.iloc[i][\"longitude\"])\n",
        "        distance_list.append(d)\n",
        "    return distance_list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xotfjIgyxLGx"
      },
      "source": [
        "However, our program seems to be taking a long time and we want to figure out if this function is causing the problem. We can check how much time this function takes with the `timeit` magic command. Magic commands are a feature of Jupyter Notebook that you use by putting either a single % in front of them (to apply them to one line) or a double %% to apply them to the whole code cell. So, let's see how long our code is taking:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ub-ot8fbxq69"
      },
      "outputs": [],
      "source": [
        "# Run the haversine looping function\n",
        "%timeit df[\"distance\"] = haversine_looping(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmXI7-PXxwCN"
      },
      "source": [
        "Is that slow? Well, this is only a very modest dataset (1631 rows), so yeah, actually that's slow. If we were only this dataset to trial our code before deploying it for a bigger problem, then we'd run into trouble. Maybe we can go faster using Pandas' built-in method to iterate over every row, called `iterrows`?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arbLjgWfyHI1"
      },
      "outputs": [],
      "source": [
        "%%timeit\n",
        "\n",
        "# Haversine applied on rows via iteration\n",
        "haversine_series = []\n",
        "for index, row in df.iterrows():\n",
        "    haversine_series.append(\n",
        "        haversine(40.671, -73.985, row[\"latitude\"], row[\"longitude\"])\n",
        "    )\n",
        "df[\"distance\"] = haversine_series\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdEFfRwcyMBM"
      },
      "source": [
        "That looks about 2.5x as fast, and we didn't have to do anything really complicated. Actually, this code looks a bit nicer. Clearly, how you iterate over a Pandas dataframe matters.\n",
        "\n",
        "Can we do even better? Yes. There are actually quite a few different methods available, ranging from quick easy fixes to more advanced ones for really heavy lifting. In the next part of this lab we'll show several approaches, but leave it to you to examine which work best."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTzQ-3bb1Ku1"
      },
      "source": [
        "#### Using `apply` and `lambda` to apply a function to each row\n",
        "Pandas has another built-in function to apply a function to a part of the dataframe, called `apply`. Apply should be called with an `axis` argument to indicate along which axis to apply the operation. Since we want to apply it to rows, we use `axis=1`.\n",
        "\n",
        "\n",
        "(The fact that the rows are axis 1, not axis 0, points to an interesting thing: Pandas is in [column major order](https://en.wikipedia.org/wiki/Row-_and_column-major_order), which implies that *any* row-wise traversal is basically going against the grain.) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kancacPyali"
      },
      "outputs": [],
      "source": [
        "df[\"distance\"] = df.apply(\n",
        "    lambda row: haversine(40.671, -73.985, row[\"latitude\"], row[\"longitude\"]), axis=1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UND2SzY1N9i"
      },
      "source": [
        "You'll also notice the word `lambda` here. Lamda creates an anonymous function for 'row' that basically wraps our haversine function. We could have also written something like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97T5c5IR1WT4"
      },
      "outputs": [],
      "source": [
        "def wrap_haversine(row):\n",
        "    return haversine(40.671, -73.985, row[\"latitude\"], row[\"longitude\"])\n",
        "\n",
        "\n",
        "df[\"distance\"] = df.apply(wrap_haversine, axis=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJ3xK3Bf1aOR"
      },
      "source": [
        "Notice that we're writing our function `wrap_haversine(row)` as `wrap_haversine`, so without any argument; that's because what we're actually doing is handing over the `apply_haversine` function itself to Pandas `apply` to apply to each row. \n",
        "\n",
        "Why did we even need this `wrap_haversine` function? Because our `haversine` function itself takes four arguments, but we only have one argument (the whole row). So, using `lambda` here is just a bit more convenient than defining a wrapper function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3M8oafQ1d1Y"
      },
      "source": [
        "#### Extracting the columns and looping over them\n",
        "We could extract the columns and loop over them; maybe that's more efficient than going against the grain of Pandas' column-major ordering?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--uO-C_Z1hea"
      },
      "outputs": [],
      "source": [
        "lat = df[\"latitude\"]\n",
        "lon = df[\"longitude\"]\n",
        "distance_list = []\n",
        "for i in range(len(lon)):\n",
        "    distance_list.append(haversine(40.671, -73.985, lat[i], lon[i]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCXz7znb1jxB"
      },
      "source": [
        "We haven't changed much from the original naive code - we simplified the lookup of the rows, now we're just using `lat[i]` to get the latitude instead of `df.iloc[i]['latitude']`, and likewise for the longitude. We paid for that with some overhead in extracting the whole column from pandas, but was it worth it?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLAeKiYt1ned"
      },
      "source": [
        "#### Getting our data column-row instead of row-column\n",
        "Building on the previous idea, maybe we'd rather not pull out those columns, but what if we just used the same looping idea, looking up columns first and then the row inside the column?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvRRBq2q1q_P"
      },
      "outputs": [],
      "source": [
        "distance_list = []\n",
        "for i in range(len(df)):\n",
        "    distance_list.append(\n",
        "        haversine(40.671, -73.985, df[\"latitude\"][i], df[\"longitude\"][i])\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkJHVPBV1sQi"
      },
      "source": [
        "#### Using the columns as raw Numpy arrays\n",
        "Pandas is built on Numpy, and you can extract a column as a Numpy array quite easily:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7X0fY9gw1uac",
        "outputId": "4f0cec57-f1e7-4480-bd0d-c1b2f25eeab7"
      },
      "outputs": [],
      "source": [
        "lon = df[\"longitude\"].values\n",
        "print(type(lon))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLQg0HJ71wdS"
      },
      "source": [
        "Can we use this to loop faster over it?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wqBCq0Q1yj2"
      },
      "outputs": [],
      "source": [
        "lat = df[\"latitude\"].values\n",
        "lon = df[\"longitude\"].values\n",
        "distance_list = []\n",
        "for i in range(len(lon)):\n",
        "    distance_list.append(haversine(40.671, -73.985, lat[i], lon[i]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPuHmgU212S8"
      },
      "source": [
        "Why should we expect looping over Numpy arrays to be faster than over a column from a Pandas array (a Pandas Series), or a classic Python list, for that matter?\n",
        "\n",
        "Pandas Series are built on Numpy arrays, but with some more indexing and other fancy features built in. Extracting the raw Numpy array just skips some of the overhead that comes with that. The difference with Python lists is much more profound. A Python list is allowed to contain any kind of object - floating point numbers, integers, strings, sub-lists, sets, dictionaries, class instances, whole classes, functions, and packages. Clearly, those different kinds of things don't all need the same kind of memory, so they can't be stored in a neat row of equal-size chunks in memory. When you decide to referce item 10 of a list, Python has to look up where item 10 is stored. Meanwhile, a Numpy array is strictly homogenous in its data type - it's all float64, or all integer32 and so forth. Each cell in the array is the same size, so if you know where the array starts, you can easily calculate where each value is. (There is more to it, but this is the key point: Numpy arrays are homogenously typed and that enables optimization tricks.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGwgGdsz14DR"
      },
      "source": [
        "### EXERCISE: which other ways work best?\n",
        "Use `%timeit` with the new methods described above and figure out which ones work best. Which ones look easiest to work with?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23B5cKuh2NKR"
      },
      "source": [
        "#### Vectorized operations\n",
        "A phrase you'll run into when you dig into Numpy, or if you're familiar with MATLAB, is vectorizing operations. Basically, it means applying an operation across an entire vector of elements all at once, rather than using a loop to process each element separately. Instead of an explicit loop, we use optimized C code built into Numpy that can apply operations to a whole vector all at once.\n",
        "\n",
        "To see this in action, we're going to use the second magic command we prepared: `lprun`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFdVZnMq2YXu"
      },
      "outputs": [],
      "source": [
        "# Haversine applied on rows with line profiler\n",
        "%lprun -f haversine df.apply(lambda row: haversine(40.671, -73.985, row[\"latitude\"], row[\"longitude\"]), axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "or_uY_J82dUP"
      },
      "source": [
        "This opens a popup window showing how many times each line in our haversine function has been hit, and how much time has been spend there. We see that we're hitting each line 1631 times, because we're calling the function for each of the 1631 rows in our dataframe. That's clearly some overhead we might be able to get rid of.\n",
        "\n",
        "Because Pandas Series columns are Numpy arrays under the hood, and because our haversine function uses suitable operations, we're able to easily vectorize our function. All we have to do is call the function with columns as arguments instead of individual values from a row:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8Qq4dwv2fEm"
      },
      "outputs": [],
      "source": [
        "# Vectorized implementation of Haversine applied on Pandas series\n",
        "%lprun -f haversine df[\"distance\"] = haversine(40.671, -73.985, df[\"latitude\"], df[\"longitude\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6eYH6dP2ijL"
      },
      "source": [
        "Looking at the profiling of this version, we now see that we're hitting each line in the function only once. Is it faster?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHj5pIfw2kXZ"
      },
      "outputs": [],
      "source": [
        "%timeit df[\"distance\"] = haversine(40.671, -73.985, df[\"latitude\"], df[\"longitude\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tya9Z0DzC2Xw"
      },
      "source": [
        "Yeah, that's quite a speedup. Of course, the trick we did before, using the raw Numpy values under the Pandas Series, we can combine that:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FjNhc-jC3oo"
      },
      "outputs": [],
      "source": [
        "%timeit df[\"distance\"] = haversine(40.671, -73.985, df[\"latitude\"].values, df[\"longitude\"].values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAXgE-Br2odo"
      },
      "source": [
        "That's even faster. At this point, we should have mostly solved our performance problems, if you can vectorize your operations this neatly, working with Pandas shouldn't be slow at all.\n",
        "\n",
        "So *if* you can vectorize your functions. What does it take to vectorize a function? Basically, you have to build it using only components that Numpy can apply to a whole vector at a time. For example, the `np.sqrt(x)` function can take the square roots of all the values in a vector all at once.\n",
        "\n",
        "Vectorizing mathematical operations like addition, subtraction, multiplication, division and so forth is generally straightforward; you might not even realize it but when you write `my_array + my_other_array`, you're using Numpy's addition code which can add two arrays together. For classic linear algebra tasks it's generally a bit more obvious because those transformations have been built as Numpy functions.\n",
        "\n",
        "A good introductory tutorial is [Look Ma, No For-Loops: Array Programming With NumPy](https://realpython.com/numpy-array-programming/), and a somewhat more advanced tutorial is [“Vectorized” Operations: Optimized Computations on NumPy Arrays](https://www.pythonlikeyoumeanit.com/Module3_IntroducingNumpy/VectorizedOperations.html).\n",
        "\n",
        "You'll see that vectorization often requires some creative rethinking of your problem, often coming at it sideways. It isn't always necessary to use vectorization, because sometimes the performance hit from a not-too-naive loop is acceptable and rebuilding your function to be vectorized takes too much work. However, if you run into serious performance problems, vectorization can be the answer. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esam0qjK2tlr"
      },
      "source": [
        "### Exercise: vectorizing your operations\n",
        "We start with two vectors of random numbers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUOIzbJYzDcF"
      },
      "outputs": [],
      "source": [
        "a = np.random.random(10 ** 7)\n",
        "b = np.random.random(10 ** 7)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWgrpV42zHuj"
      },
      "source": [
        "Now we're computing the dot product, that is, the sum of multiplying the elements along the two vectors. This has been coded as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6mp5mySzKMk"
      },
      "outputs": [],
      "source": [
        "%%timeit\n",
        "\n",
        "dot_product = 0\n",
        "for i in range(len(a)):\n",
        "    dot_product += a[i] * b[i]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ksyz7RfJzMwR"
      },
      "source": [
        "**YOUR TASK** vectorize this code and compare the difference in speed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgUk--ErzQAC"
      },
      "outputs": [],
      "source": [
        "%%timeit\n",
        "dot_product = np.dot(a, b)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHpYborw2zDT"
      },
      "source": [
        "#### Pushing the envelope: optimizing your function line by line\n",
        "If you've already done vectorization, and performance is still not satisfactory, you can also use the line profiler to look at the lines of your code and see where the most time is spent. For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6sMK3bX21gm"
      },
      "outputs": [],
      "source": [
        "# Vectorized implementation of Haversine applied on Pandas series\n",
        "%lprun -f haversine df[\"distance\"] = haversine(40.671, -73.985, df[\"latitude\"].values, df[\"longitude\"].values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuCyLwwk23A6"
      },
      "source": [
        "We see that most time is spent in the line `a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2`. If we wanted to try more optimization, that would be were we should focus our efforts.\n",
        "\n",
        "Doing so is beyond the scope of this tutorial, but it doesn't actually require you to start writing C code. You could use Cython or Numexpr to speed things up, as shown in this tutorial: [Enhancing performance](https://pandas.pydata.org/pandas-docs/stable/user_guide/enhancingperf.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tr3Glylb45Fg"
      },
      "source": [
        "### Matplotlib tips and tricks\n",
        "Consider the code and plot below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2NOgHBY47lz"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(15, 10))\n",
        "\n",
        "# generate some dummy data\n",
        "x = np.linspace(-10, 10, 41)  # 41 equally spaced points in [-10, 10]\n",
        "y = x ** 2\n",
        "\n",
        "# plot the data\n",
        "ax.plot(x, y, label=\"label for my line\", color=\"red\")\n",
        "\n",
        "# make the plot look good\n",
        "ax.set_title(\"my title\")\n",
        "ax.set_xlabel(\"my x label\")\n",
        "ax.set_ylabel(\"my y label\")\n",
        "ax.legend()  # display the legend; matplotlib tries to put it somewhere convenient\n",
        "ax.set_xticks(np.linspace(-10, 10, 21))  # set the ticks along the x-axis\n",
        "ax.set_yscale(\"log\")  # display the y dimension in a log scale\n",
        "ax.grid()  # turn on the gridlines\n",
        "\n",
        "# save plots before showing, otherwise you get empty pictures\n",
        "fig.savefig(\"plots/A.png\")\n",
        "fig.savefig(\"plots/B.jpg\")\n",
        "fig.savefig(\"plots/C.svg\")\n",
        "fig.savefig(\"plots/D.pdf\")\n",
        "fig.savefig(\"plots/E.eps\")\n",
        "fig.savefig(\"plots/F\")\n",
        "\n",
        "# now show it\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETPwe3dM5DVt"
      },
      "source": [
        "Consider these questions:\n",
        "\n",
        "* What is the default file format that Matplotlib saves in?\n",
        "* Which file format looks good if you enlarge it (especially, if you integrate it in a report)?\n",
        "* Which file format leaves text as text? This is handy if people want to use copy-paste, or a screen reader for people with disabilities. Or if they still want to edit the graph (e.g., axis labels, sizes, etc.) using another tool.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqbnvLHf5FF8"
      },
      "source": [
        "#### Axes and Figures\n",
        "\n",
        "You'll also notice that this plot uses a lot of calls on the `ax` object and a few on `fig`, but not a lot of calls to `plt`. In a lot of tutorials you'll see most of the same code, but all calls made to `plt`. Why is that?\n",
        "\n",
        "Basically, Matplotlib got started as a port of MATLAB plotting capabilities to Python. `plt` is basically a placeholder that points to the most recent figure you were working on. That's fine as long as you're only working on one plot in a project, but it can become confusing if you're generating a lot of pictures.\n",
        "\n",
        "What we do when we call `fig, ax = plt.subplots()` is generate a new *Figure* with in this case one *Axes* object. The Axes object is what we think of as the actual plot: it has an X and Y axis (hence the name, axes, plural of axis), and it's where we're drawing on. The Figure is more like the windowframe around it. We can have a Figure with multiple Axes in it, which is what happens if you call [plt.subplots](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplots.html) with arguments to indicate how many you'd like; then instead of a single Axes object, you get a bunch of them.  For a more extensive tutorial of \"best practices\" Matplotlib, I recommend [Python Plotting With Matplotlib (Guide)](https://realpython.com/python-matplotlib-guide/) and [Effectively Using Matplotlib](https://pbpython.com/effective-matplotlib.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uihCa77W5y4t"
      },
      "source": [
        "### Exercise: a larger scale experiment\n",
        "This is the last task in this lab. Try to think about how you'd set this up if you had to run variants of these experiments many, many times, and you didn't want to have to do everything by hand every time. \n",
        "\n",
        "You've already compared a number of optimization techniques, but you've only compared them on one dataset of a specific size. But the method that looks good on this dataset might not scale well to bigger datasets (maybe it's not linear..), so we want to test these methods on multiple sizes of datasets.\n",
        "\n",
        "**YOUR TASK**\n",
        "* Choose four different ways of using the haversine function.\n",
        "* Apply them to this dataset, but scaled up to 2x, 5x, 10, 20x, 50x and 100x the original size.\n",
        "* Plot the results with Matplotlib. Experiment with the way you set up the plot to get a picture that is useful for analyzing your results.\n",
        "* Try to find out if the runtime  of any of the methods doesn't linearly scale with the size of the dataset. Does it scale better (i.e. less fast) or worse?\n",
        "\n",
        "You can enlarge a dataset by just duplicating the rows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pz7ol9_-5301"
      },
      "outputs": [],
      "source": [
        "df2 = df.append([df], ignore_index=True)\n",
        "df5 = df.append([df] * 4, ignore_index=True)\n",
        "df10 = df.append([df] * 9, ignore_index=True)\n",
        "df20 = df.append([df] * 19, ignore_index=True)\n",
        "df100 = df.append([df] * 99, ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aR-n7KTM55ir"
      },
      "source": [
        "You can also extract the information we collect by `%timeit` so that you can use it for plotting:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2e5B8KQ57oV"
      },
      "outputs": [],
      "source": [
        "res = %timeit -o df['distance'] = df.apply(lambda row: haversine(40.671, -73.985, row['latitude'], row['longitude']), axis=1)\n",
        "runtime = np.mean(res.all_runs)  # measured in seconds\n",
        "print(\"mean runtime (s):\", runtime)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First we start by defining different functions that apply the haversine function to a dataframe.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def simple_loop(df: pd.DataFrame) -> pd.Series:\n",
        "    distances = []\n",
        "    for i in range(0, len(df)):\n",
        "        d = haversine(40.671, -73.985, df.iloc[i]['latitude'], df.iloc[i]['longitude'])\n",
        "        distance_list.append(d)\n",
        "    return pd.Series(distance_list)\n",
        "\n",
        "def iterrows_loop(df: pd.DataFrame) -> pd.Series:\n",
        "    distances = []\n",
        "    for i, row in df.iterrows():\n",
        "        d = haversine(40.671, -73.985, row['latitude'], row['longitude'])\n",
        "        distance_list.append(d)\n",
        "    return pd.Series(distance_list)\n",
        "\n",
        "def loop_col_first(df: pd.DataFrame) -> pd.Series:\n",
        "    distance_list = []\n",
        "    for i in range(len(df)):\n",
        "        distance_list.append(haversine(40.671, -73.985, df[\"latitude\"][i], df[\"longitude\"][i]))\n",
        "    return pd.Series(distance_list)\n",
        "\n",
        "def apply_lambda(df: pd.DataFrame) -> pd.Series:\n",
        "    return df.apply(lambda row: haversine(40.671, -73.985, row['latitude'], row['longitude']), axis=1)\n",
        "\n",
        "def apply_predefined(df: pd.DataFrame) -> pd.Series:\n",
        "    def haversine_wrapper(row: tuple) -> float:\n",
        "        return haversine(40.671, -73.985, row['latitude'], row['longitude'])\n",
        "    return df.apply(haversine_wrapper, axis=1)\n",
        "\n",
        "def pd_vectorized(df: pd.DataFrame) -> pd.Series:\n",
        "    return haversine(40.671, -73.985, df[\"latitude\"], df[\"longitude\"])\n",
        "\n",
        "def np_vectorized(df: pd.DataFrame) -> pd.Series:\n",
        "    return pd.Series(haversine(40.671, -73.985, df['latitude'].values, df['longitude'].values))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we time them.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "approaches = [simple_loop, iterrows_loop, loop_col_first, apply_lambda, apply_predefined, pd_vectorized, np_vectorized]\n",
        "dfs = [df, df2, df5, df10, df20, df100]\n",
        "\n",
        "time_per_approach = {}\n",
        "for fn in approaches:\n",
        "    runtimes = []\n",
        "    for df in dfs:\n",
        "        runtime = %timeit -o -q fn(df)\n",
        "        runtimes.append(runtime.average)\n",
        "    time_per_approach.update({fn.__name__: runtimes})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = [df.shape[0] for df in dfs]\n",
        "\n",
        "fig, axes = plt.subplots(\n",
        "    ncols = 2,\n",
        "    sharey = True,\n",
        "    figsize = (15, 5)\n",
        ")\n",
        "\n",
        "for label, runtimes in time_per_approach.items():\n",
        "    for ax in axes:\n",
        "        ax.plot(\n",
        "            x,\n",
        "            runtimes,\n",
        "            label = label,\n",
        "            marker = 'o'\n",
        "        )\n",
        "\n",
        "axes[0].legend()\n",
        "axes[1].set_xscale('log')\n",
        "\n",
        "fig.suptitle('Runtimes of different dataframe mapping approaches', weight='bold')\n",
        "fig.supxlabel('rows in dataframe')\n",
        "fig.supylabel('runtime (s)')\n",
        "fig.tight_layout(w_pad=.5)\n",
        "fig.savefig('plots/runtimes.png', dpi=500)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that at least all non-vectorized approaches scale linearly.\n",
        "From this figure it is hard to draw any meaningful conclusion on the scaling of the two most efficient approaches (`pd_vectorized` & `np_vectorized`) as the increments are too small.\n",
        "Another interesting thing to explore would be to see which of the two \"apply\" methods is most efficient, even though the difference seems trivial.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First we will examine the \"vectorized\" methods in isolation, and plotting them against a standard linear x axis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(8, 5))\n",
        "\n",
        "time_per_vectorized_approach = {va: time_per_approach[va] for va in ['pd_vectorized', 'np_vectorized']}\n",
        "\n",
        "for label, runtimes in time_per_vectorized_approach.items():\n",
        "    ax.plot(\n",
        "        x,\n",
        "        runtimes,\n",
        "        label=label,\n",
        "        marker='o'\n",
        "    )\n",
        "\n",
        "ax.legend()\n",
        "ax.set_title('Runtime comparison for vectorized approaches', weight='bold')\n",
        "ax.set_xlabel('rows in dataframe')\n",
        "ax.set_ylabel('runtime (s)')\n",
        "fig.savefig('plots/runtimes_v.png', dpi=500)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It appears that this approach too, scales linearly.\n",
        "We also see that using the numpy arrays extracted from the pandas series objects is indeed more efficient than using the series objects directly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now do a similar thing for the \"apply\" approaches, i.e. the approaches where we use the dataframe's `apply` method.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(8, 5))\n",
        "\n",
        "time_per_vectorized_approach = {va: time_per_approach[va] for va in ['apply_lambda', 'apply_predefined']}\n",
        "\n",
        "for label, runtimes in time_per_vectorized_approach.items():\n",
        "    ax.plot(\n",
        "        x,\n",
        "        runtimes,\n",
        "        label=label,\n",
        "        marker='o'\n",
        "    )\n",
        "\n",
        "ax.legend()\n",
        "ax.set_title('Runtime comparison for \"apply\" approaches', weight='bold')\n",
        "ax.set_xlabel('rows in dataframe')\n",
        "ax.set_ylabel('runtime (s)')\n",
        "fig.savefig('plots/runtimes_a.png', dpi=500)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This still does not tell us much, so now we will plot only the differences.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "time_per_vectorized_approach = {va: time_per_approach[va] for va in ['apply_lambda', 'apply_predefined']}\n",
        "\n",
        "if sum(time_per_approach['apply_predefined']) < sum(time_per_approach['apply_lambda']):\n",
        "    best, not_best = 'apply_predefined', 'apply_lambda'\n",
        "else:\n",
        "    best, not_best = 'apply_lambda', 'apply_predefined'\n",
        "\n",
        "diffs = (np.array(time_per_approach[not_best]) - np.array(time_per_approach[best])) * 1000\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 5))\n",
        "\n",
        "ax.plot(\n",
        "    x,\n",
        "    diffs,\n",
        "    marker='o'\n",
        ")\n",
        "\n",
        "ax.set_title(f'Overhead introduced by using \"{not_best}\"', weight='bold')\n",
        "ax.set_xlabel('rows in dataframe')\n",
        "ax.set_ylabel('overhead (ms)')\n",
        "fig.savefig('plots/runtimes_d.png', dpi=500)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There is some interesting scaling going here, specifically the peak at ±35000 entries.\n",
        "This could however just be attributed to a margin of error, as the differences are never more than 10 milliseconds.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QleIYUm9589o"
      },
      "source": [
        "### Reflection\n",
        "Consider the following questions:\n",
        "* What steps can you take to ensure your functions can be vectorized?\n",
        "* If your function is too hard to vectorize, how can you best loop over your dataframe?\n",
        "* How can you track down performance bottlenecks in your code?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOs1FtQt6Ari"
      },
      "source": [
        "### References\n",
        "\n",
        "#### Development Containers\n",
        "- [Beginner's Series to Dev Containers](https://www.youtube.com/watch?v=61M2takIKl8&list=PLj6YeMhvp2S5G_X6ZyMc8gfXPMFPg3O31) on Youtube\n",
        "\n",
        "#### Numpy & Pandas\n",
        "* [Cloud X Lab - introduction to Numpy and Pandas](https://cloudxlab.com/blog/numpy-pandas-introduction/)\n",
        "* [Zero With Dot - Performance of numpy and pandas - comparison](https://zerowithdot.com/python-numpy-and-pandas-performance/)\n",
        "* [Sofia Heisler - A Beginner’s Guide to Optimizing Pandas Code for Speed](https://engineering.upside.com/a-beginners-guide-to-optimizing-pandas-code-for-speed-c09ef2c6a4d6) basic optimization techniques, see also her PyCon 2017 talk on [YouTube](https://www.youtube.com/watch?v=HN5d490_KKk).\n",
        "* [Pandas - Enhancing Performance](https://pandas.pydata.org/pandas-docs/stable/user_guide/enhancingperf.html) deeper enhancement techniques\n",
        "* [Numexpr](https://github.com/pydata/numexpr) easily speed up more complex numpy operations\n",
        "* [Real Python - Look Ma, No For-Loops: Array Programming With NumPy](https://realpython.com/numpy-array-programming/) a somewhat easier introduction to vectorization\n",
        "* [Python Like You Mean It - Vectorized Operations](https://www.pythonlikeyoumeanit.com/Module3_IntroducingNumpy/VectorizedOperations.html) a bit more advanced text about vectorization\n",
        "\n",
        "#### Matplotlib\n",
        "* [Real Python - Python Plotting With Matplotlib (Guide)](https://realpython.com/python-matplotlib-guide/)\n",
        "* [Practical Business Python - Effectively Using Matplotlib](https://pbpython.com/effective-matplotlib.html)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Lab1_Exercises.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.2 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
